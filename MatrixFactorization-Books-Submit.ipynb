{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b58dff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample of the 'Books and Book-Titles' dataframe\n",
      "\n",
      "      book_id                                              title\n",
      "8419     8420                               Batman: Hush, Vol. 1\n",
      "435       436                            The God of Small Things\n",
      "5520     5521           The Six Sacred Stones (Jack West Jr, #2)\n",
      "1861     1862  Heir to the Empire (Star Wars: The Thrawn Tril...\n",
      "3286     3287                      Food Rules: An Eater's Manual\n",
      "\n",
      "\n",
      "Sample of the 'User-ID ratings of Book-ID' dataframe\n",
      "\n",
      "         user_id  book_id  rating\n",
      "1558732    12445      576       3\n",
      "5837973    29797       24       4\n",
      "2103226    28350     7301       2\n",
      "5218814    29069     1728       5\n",
      "4688029    49783     5065       3\n",
      "Shape User-Ratings unfiltered:\t(5976479, 3)\n",
      "Shape User-Ratings filtered:\t(4508993, 3)\n",
      "\n",
      " unique user_id counts: 36199\n",
      "\n",
      " unique book_id counts: 9466\n",
      "Shape of training set: (3508993, 3) Shape of testing set: (1000000, 3)\n",
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "book (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding (Embedding)      (None, 1, 100)       3619900     user[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "book_embedding (Embedding)      (None, 1, 100)       946600      book[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 100)          0           user_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, 100)          0           book_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1)            0           reshape_2[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,566,500\n",
      "Trainable params: 4,566,500\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "3085/3085 [==============================] - 119s 39ms/step - loss: 4.7032 - val_loss: 10.3896\n",
      "Epoch 2/5\n",
      "3085/3085 [==============================] - 124s 40ms/step - loss: 0.7400 - val_loss: 10.1715\n",
      "Epoch 3/5\n",
      "3085/3085 [==============================] - 117s 38ms/step - loss: 0.6695 - val_loss: 10.1006\n",
      "Epoch 4/5\n",
      "3085/3085 [==============================] - 117s 38ms/step - loss: 0.6036 - val_loss: 10.0698\n",
      "Epoch 5/5\n",
      "3085/3085 [==============================] - 117s 38ms/step - loss: 0.5298 - val_loss: 10.0560\n",
      "\n",
      "\n",
      "Testing Result With DL Matrix-Factorization: 2.5370 RMSE\n",
      "First 20 rows:\n",
      "\n",
      "\n",
      "    User ID  Book ID                                          Book Name  \\\n",
      "0     32352     1488                                       Go, Dog. Go!   \n",
      "1     32352      232                The Gunslinger (The Dark Tower, #1)   \n",
      "2     32352      225                                       East of Eden   \n",
      "3     32352      493                                  Mere Christianity   \n",
      "4     32352     6074                        Surface Detail (Culture #9)   \n",
      "5     32352       83                               A Tale of Two Cities   \n",
      "6     32352     1286                         Quidditch Through the Ages   \n",
      "7     32352      534                    When You Are Engulfed in Flames   \n",
      "8     14257     8874                                           Tell-All   \n",
      "9     32352      831                                       Lock and Key   \n",
      "10    32352      584                   The Magicians (The Magicians #1)   \n",
      "11    32352      423                      The Elite (The Selection, #2)   \n",
      "12    32352     1985                                   The Grand Design   \n",
      "13    32352      421                                     The Paris Wife   \n",
      "14    32352      806              Wizard and Glass (The Dark Tower, #4)   \n",
      "15    32352     2300          Locke & Key, Vol. 1: Welcome to Lovecraft   \n",
      "16    32352     2436  Touching the Void: The True Story of One Man's...   \n",
      "17    32352     3475                                         Look Again   \n",
      "18    32352      472                           Man's Search for Meaning   \n",
      "19    32352     2002                            The Death of Ivan Ilych   \n",
      "\n",
      "    Predicted Rating  Actual Rating  \n",
      "0               1.00              3  \n",
      "1               1.00              5  \n",
      "2               1.00              5  \n",
      "3               1.00              5  \n",
      "4               1.00              3  \n",
      "5               1.00              5  \n",
      "6               1.00              5  \n",
      "7               1.00              5  \n",
      "8               2.16              4  \n",
      "9               1.00              4  \n",
      "10              1.00              4  \n",
      "11              1.00              2  \n",
      "12              1.00              2  \n",
      "13              1.00              4  \n",
      "14              1.00              4  \n",
      "15              1.00              5  \n",
      "16              1.00              4  \n",
      "17              1.00              5  \n",
      "18              1.00              5  \n",
      "19              1.00              5  \n",
      "\n",
      "\n",
      " 20 random rows:\n",
      "\n",
      "\n",
      "        User ID  Book ID                                          Book Name  \\\n",
      "99961     32835     5783                                             Island   \n",
      "92824     13996     8674                  Infamous (Chronicles of Nick, #3)   \n",
      "675296     4048     5804                           Sometimes a Great Notion   \n",
      "46878     32638     8646              Spying in High Heels (High Heels, #1)   \n",
      "224497    28095     7559                         The Thing Around Your Neck   \n",
      "340855    25649     9009  My Life with the Walter Boys (My Life with the...   \n",
      "416254    34738      116                       The Adventures of Tom Sawyer   \n",
      "447072    15975      233                        Love in the Time of Cholera   \n",
      "386953    34592     8899                                     Selected Poems   \n",
      "242991    12787     2013                                           Saturday   \n",
      "214951    31143     7023                              Private (Private, #1)   \n",
      "81389     32851       88                                        Paper Towns   \n",
      "365883     8526     3211                            Thanks for the Memories   \n",
      "16430     10112     8565                                           Nostromo   \n",
      "57572     32707     7099         The Sentry (Elvis Cole, #14; Joe Pike, #3)   \n",
      "238336    33757      408  Guns, Germs, and Steel: The Fates of Human Soc...   \n",
      "742079    20425      143                        All the Light We Cannot See   \n",
      "333333    34302      442                                         Snow Crash   \n",
      "95377     32936     4421                                               Pulp   \n",
      "955224    31669     2467                                        Little Bear   \n",
      "\n",
      "        Predicted Rating  Actual Rating  \n",
      "99961              1.000              5  \n",
      "92824              3.373              5  \n",
      "675296             3.478              4  \n",
      "46878              1.000              5  \n",
      "224497             4.115              5  \n",
      "340855             1.000              3  \n",
      "416254             1.000              3  \n",
      "447072             3.706              3  \n",
      "386953             1.000              5  \n",
      "242991             3.927              5  \n",
      "214951             1.000              4  \n",
      "81389              1.000              4  \n",
      "365883             3.517              4  \n",
      "16430              5.000              3  \n",
      "57572              1.000              4  \n",
      "238336             1.000              4  \n",
      "742079             3.800              3  \n",
      "333333             1.000              5  \n",
      "95377              1.000              3  \n",
      "955224             1.000              3  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a user-id from 1 to 36198 (0 to 'quit'): 5000\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Books User  5000  will LOVE\n",
      "\n",
      "The Gunslinger (The Dark Tower, #1)                                     4.66\n",
      "Marlfox (Redwall, #11)                                                  4.60\n",
      "The Day the Crayons Quit                                                4.49\n",
      "In the Garden of Beasts: Love, Terror, and an American Family in        4.47\n",
      "Then He Ate My Boy Entrancers (Confessions of Georgia Nicolson, #       4.44\n",
      "An Anthropologist on Mars: Seven Paradoxical Tales                      4.44\n",
      "A Streetcar Named Desire                                                4.43\n",
      "Home                                                                    4.39\n",
      "The Hidden Child (Patrik Hedström, #5)                                  4.36\n",
      "Green Mars (Mars Trilogy, #2)                                           4.34\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Books User  5000  will LIKE\n",
      "\n",
      "The Hedge Knight (The Tales of Dunk and Egg, #1)                        3.99\n",
      "The Last Kingdom (The Saxon Stories, #1)                                3.99\n",
      "The Alchemist                                                           3.99\n",
      "The Bourne Identity (Jason Bourne, #1)                                  3.99\n",
      "The Unlikely Spy                                                        3.98\n",
      "Brighton Rock                                                           3.98\n",
      "False Memory                                                            3.98\n",
      "The Help                                                                3.98\n",
      "The Silence of the Lambs  (Hannibal Lecter, #2)                         3.98\n",
      "Avatar: The Last Airbender (The Rift, #1)                               3.98\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Books User  5000  will consider 'SO-SO'\n",
      "\n",
      "Bastard Out of Carolina                                                 2.99\n",
      "Shut Out                                                                2.99\n",
      "The Elite (The Selection, #2)                                           2.99\n",
      "A Christmas Carol                                                       2.99\n",
      "Kitty and the Midnight Hour (Kitty Norville #1)                         2.99\n",
      "The Voyage of the Dawn Treader (Chronicles of Narnia, #3)               2.99\n",
      "The Night Strangers                                                     2.99\n",
      "How Google Works                                                        2.99\n",
      "Scarpetta (Kay Scarpetta, #16)                                          2.99\n",
      "Veil of Midnight (Midnight Breed, #5)                                   2.99\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Books User  5000  will DISLIKE\n",
      "\n",
      "The Queen's Poisoner (Kingfountain, #1)                                 1.99\n",
      "The Magician's Nephew (Chronicles of Narnia, #6)                        1.99\n",
      "Gaudy Night (Lord Peter Wimsey, #12)                                    1.98\n",
      "Magical Thinking: True Stories                                          1.98\n",
      "Brain on Fire: My Month of Madness                                      1.98\n",
      "Sarah's Key                                                             1.98\n",
      "Megan Meade's Guide to the McGowan Boys                                 1.97\n",
      "رأيت رام الله                                                           1.97\n",
      "It's Kind of a Funny Story                                              1.97\n",
      "Drift: The Unmooring of American Military Power                         1.97\n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Books User  5000  will HATE\n",
      "\n",
      "The Psychopath Test: A Journey Through the Madness Industry             0.82\n",
      "How to Be a Domestic Goddess: Baking and the Art of Comfort Cooki       0.80\n",
      "Tell the Wolves I'm Home                                                0.18\n",
      "Girls in Love (Girls, #1)                                               0.14\n",
      "Life, the Universe and Everything (Hitchhiker's Guide, #3)              0.14\n",
      "Hollywood Dirt (Hollywood Dirt, #1)                                     0.13\n",
      "Unseen (Will Trent, #7)                                                 0.13\n",
      "The Staff of Serapis (Percy Jackson & Kane Chronicles Crossover #       0.13\n",
      "The Red Tent                                                            0.13\n",
      "Richard II                                                              0.12\n",
      "Enter a user-id from 1 to 36198 (0 to 'quit'): 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# first show the books they've already read (and rated)\\ndf_the_user = df_filtered[df_filtered['user_id'] == the_user_id]\\nprint(df_the_user)\\n\\n# this user has read 111 of the 9466 books.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To store\\load the data\n",
    "import pandas as pd\n",
    "\n",
    "# To do linear algebra\n",
    "import numpy as np\n",
    "\n",
    "# To compute similarities between vectors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# To create deep learning models\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.layers import Input, Embedding, Reshape, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Load the 'Books and Book-Titles' \n",
    "df_books_and_titles = pd.read_csv('books.csv')\n",
    "df_books_and_titles = df_books_and_titles[['book_id','title']]\n",
    "print(\"\\n\\nSample of the 'Books and Book-Titles' dataframe\\n\")\n",
    "print(df_books_and_titles.sample(5))\n",
    "\n",
    "# Create a dictionary mapping 'book_id' to 'title'\n",
    "di_book_title = {}\n",
    "for i in range(len(df_books_and_titles)):\n",
    "    bk_id = df_books_and_titles.loc[i,'book_id']\n",
    "    title = df_books_and_titles.loc[i,'title']\n",
    "    di_book_title[bk_id] = title\n",
    "    \n",
    "# Load the 'User-ID ratings of Book-ID'\n",
    "df_user_book_ratings = pd.read_csv('ratings-books.csv')\n",
    "print(\"\\n\\nSample of the 'User-ID ratings of Book-ID' dataframe\\n\")\n",
    "print(df_user_book_ratings.sample(5))\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Filter sparse movies\n",
    "min_book_ratings = 100\n",
    "filter_books = (df_user_book_ratings['book_id'].value_counts()>min_book_ratings)\n",
    "filter_books = filter_books[filter_books].index.tolist()\n",
    "\n",
    "# Filter sparse users\n",
    "min_user_ratings = 100\n",
    "filter_users = (df_user_book_ratings['user_id'].value_counts()>min_user_ratings)\n",
    "filter_users = filter_users[filter_users].index.tolist()\n",
    "\n",
    "# Actual filtering\n",
    "df_filtered = df_user_book_ratings[(df_user_book_ratings['book_id'].isin(filter_books)) & \\\n",
    "                                   (df_user_book_ratings['user_id'].isin(filter_users))]\n",
    "\n",
    "del filter_books, filter_users, min_book_ratings, min_user_ratings\n",
    "print('Shape User-Ratings unfiltered:\\t{}'.format(df_user_book_ratings.shape))\n",
    "print('Shape User-Ratings filtered:\\t{}'.format(df_filtered.shape))\n",
    "\n",
    "print(\"\\n unique user_id counts:\", len(df_filtered.groupby(['user_id']).count()))\n",
    "print(\"\\n unique book_id counts:\", len(df_filtered.groupby(['book_id']).count()))\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Testingsize\n",
    "n = 1000000\n",
    "\n",
    "# Split train- & testset\n",
    "df_train = df_filtered[:-n]\n",
    "df_test = df_filtered[-n:]\n",
    "print(\"Shape of training set:\",df_train.shape, \"Shape of testing set:\",df_test.shape)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Create user and movie-id mapping to convert to numbers\n",
    "user_id_mapping = {id:i for i, id in enumerate(df_filtered['user_id'].unique())}\n",
    "#print(user_id_mapping) # user_id_mapping is a dictionary that simply re-enumerates userIDs to sequential numbers 0,1,2,3\n",
    "book_id_mapping = {id:i for i, id in enumerate(df_filtered['book_id'].unique())}\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# use dataframe map function to map users & movies to mapped ids based on above mapping\n",
    "train_user_data = df_train['user_id'].map(user_id_mapping)\n",
    "train_book_data = df_train['book_id'].map(book_id_mapping)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# do the same for test data\n",
    "test_user_data = df_test['user_id'].map(user_id_mapping)\n",
    "test_book_data = df_test['book_id'].map(book_id_mapping)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Get input variable-sizes\n",
    "users = len(user_id_mapping)\n",
    "books = len(book_id_mapping)\n",
    "embedding_size = 100\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# use Input() to create tensors for - 'user' and 'movie'\n",
    "user_id_input = Input(shape=(1,), name='user')\n",
    "book_id_input = Input(shape=(1,), name = 'book')\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Create embedding layer for users \n",
    "user_embedding = Embedding(output_dim=embedding_size, \n",
    "                           input_dim=users,\n",
    "                           input_length=1, \n",
    "                           name='user_embedding')(user_id_input)\n",
    "\n",
    "# create embedding layer for movies just like users\n",
    "book_embedding = Embedding(output_dim=embedding_size, \n",
    "                           input_dim=books,\n",
    "                           input_length=1, \n",
    "                           name='book_embedding')(book_id_input)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Reshape the embedding layers\n",
    "user_vector = Reshape([embedding_size])(user_embedding)\n",
    "\n",
    "book_vector = Reshape([embedding_size])(book_embedding)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Compute dot-product of reshaped embedding layers as prediction\n",
    "y = Dot(1, normalize=False)([user_vector, book_vector])\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Setup model\n",
    "model = Model(inputs=[user_id_input, book_id_input], outputs=y)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Fit model\n",
    "X = [train_user_data, train_book_data]\n",
    "y = df_train['rating']\n",
    "\n",
    "batch_size = 1024\n",
    "epochs = 5\n",
    "validation_split = 0.1\n",
    "\n",
    "model.fit(X, y,\n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs,\n",
    "          validation_split=validation_split,\n",
    "          shuffle=True,\n",
    "          verbose=1)\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "# Test model by making predictions on test data\n",
    "y_pred = model.predict([test_user_data, test_book_data]).ravel()\n",
    "# clip upper and lower ratings\n",
    "y_pred = list(map(lambda x: 1.0 if x < 1 else 5.0 if x > 5.0 else x, y_pred))\n",
    "# get true labels\n",
    "y_true = df_test['rating'].values\n",
    "\n",
    "#  Compute RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n",
    "print('\\n\\nTesting Result With DL Matrix-Factorization: {:.4f} RMSE'.format(rmse))\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "di_book_title = {}\n",
    "for i in range(len(df_books_and_titles)):\n",
    "    bk_id = df_books_and_titles.loc[i,'book_id']\n",
    "    title = df_books_and_titles.loc[i,'title']\n",
    "    di_book_title[bk_id] = title\n",
    "\n",
    "lst_titles = []\n",
    "for i in test_book_data.values:\n",
    "    if i > 0:\n",
    "        tit = di_book_title[i]\n",
    "        lst_titles.append(tit)\n",
    "    else:\n",
    "        lst_titles.append(\"N/A\")\n",
    "    \n",
    "results_df = pd.DataFrame({\n",
    "    'User ID': test_user_data.values,\n",
    "    'Book ID': test_book_data.values,\n",
    "    'Book Name' : lst_titles,\n",
    "    'Predicted Rating': np.round(y_pred, 3),\n",
    "    'Actual Rating': y_true\n",
    "})\n",
    "\n",
    "print(\"First 20 rows:\\n\\n\")\n",
    "print(results_df.head(20))\n",
    "print(\"\\n\\n 20 random rows:\\n\\n\")\n",
    "print(results_df.sample(20))\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "\n",
    "def get_user_id():\n",
    "    x = int(input(\"Enter a user-id from 1 to 36198 (0 to 'quit'): \"))\n",
    "    if x < 1 or x > 36198:\n",
    "        return int(0)\n",
    "    else:\n",
    "        return int(x)\n",
    "\n",
    "# now sort them\n",
    "def Sort_Tuple(tup): \n",
    "  \n",
    "    # reverse = None (Sorts in Ascending order) \n",
    "    # key is set to sort using second element of \n",
    "    # sublist lambda has been used \n",
    "    tup.sort(key = lambda x: x[1],reverse = True) \n",
    "    return tup \n",
    "\n",
    "def print_books_with_this_rating(id,top_rating):\n",
    "\n",
    "    the_user_vector = model.get_layer('user_embedding').get_weights()[0][id]\n",
    "    all_book_vectors = model.get_layer('book_embedding').get_weights()[0]\n",
    "\n",
    "    lst = []\n",
    "    for i in range(len(all_book_vectors)):\n",
    "        book_vec = all_book_vectors[i]\n",
    "        x = np.dot(the_user_vector,book_vec)\n",
    "        lst.append((i,x))\n",
    "\n",
    "    sorted_lst = Sort_Tuple(lst)\n",
    "    \n",
    "    top_index = 0\n",
    "\n",
    "    for i in range(len(sorted_lst)):\n",
    "        bk_id,rating = sorted_lst[i]\n",
    "        if rating <= top_rating:  # first index that is <= top_index\n",
    "            top_index = i\n",
    "            break;    \n",
    "            \n",
    "    # print out the top 10 books for this user\n",
    "    print(\"\\n-----------------------------------------------------------------------------\\n\")\n",
    "    if top_rating >= 4.00:  #4.00 to 5.00\n",
    "        print(\"Books User \",id,\" will LOVE\\n\")\n",
    "    elif top_rating >= 3.00:  #3.00 to 3.99\n",
    "        print(\"Books User \",id,\" will LIKE\\n\")\n",
    "    elif top_rating > 2.00:  #2.00 to 2.99\n",
    "        print(\"Books User \",id,\" will consider 'SO-SO'\\n\")\n",
    "    elif top_rating > 1.00:  #1.00 to 1.99\n",
    "        print(\"Books User \",id,\" will DISLIKE\\n\")\n",
    "    else:\n",
    "        print(\"Books User \",id,\" will HATE\\n\")\n",
    "\n",
    "    num_books_to_display = 10\n",
    "    for i in range(num_books_to_display):\n",
    "        bk_id,rating = sorted_lst[top_index + i]\n",
    "        bk_title = lst_titles[bk_id]\n",
    "        if (rating <= top_rating) and ((top_rating - 1.0) <= rating):\n",
    "            print(f'{bk_title[:65].ljust(70)}  {rating:3.2f}')\n",
    "\n",
    "    \n",
    "the_user_id = 5000\n",
    "while the_user_id != 0:\n",
    "    the_user_id = get_user_id()\n",
    "    if the_user_id == 0:\n",
    "        break\n",
    "    print_books_with_this_rating(the_user_id,4.99)\n",
    "    print_books_with_this_rating(the_user_id,3.99)\n",
    "    print_books_with_this_rating(the_user_id,2.99)\n",
    "    print_books_with_this_rating(the_user_id,1.99)\n",
    "    print_books_with_this_rating(the_user_id,0.99)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "# first show the books they've already read (and rated)\n",
    "df_the_user = df_filtered[df_filtered['user_id'] == the_user_id]\n",
    "print(df_the_user)\n",
    "\n",
    "# this user has read 111 of the 9466 books.\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------- #\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a298585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
